name: Preprocess data
description: 'Preprocess data: load CSV, standardize features, split train/test, and
  save.'
inputs:
- {name: raw_path, type: String, description: Path to the raw CSV file}
- {name: processed_path, type: String, description: Path where the processed.pkl file
    will be saved}
outputs:
- {name: Output, type: String}
implementation:
  container:
    image: python:3.10
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'mlflow' 'pandas' 'numpy' 'scikit-learn' 'joblib' || PIP_DISABLE_PIP_VERSION_CHECK=1
      python3 -m pip install --quiet --no-warn-script-location 'mlflow' 'pandas' 'numpy'
      'scikit-learn' 'joblib' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def preprocess_data(raw_path , processed_path )  :
          """
          Preprocess data: load CSV, standardize features, split train/test, and save.

          Args:
              raw_path: Path to the raw CSV file
              processed_path: Path where the processed.pkl file will be saved

          Returns:
              String path of the processed file
          """
          # Convert to Path objects
          raw_path = Path(raw_path)
          processed_path = Path(processed_path)

          # Ensure destination directory exists
          processed_path.parent.mkdir(parents=True, exist_ok=True)

          # Start MLflow run
          with mlflow.start_run(run_name="preprocessing"):
              # Load CSV using pandas
              df = pd.read_csv(raw_path)

              # Separate features and target (assuming 'medv' is the target column)
              # For Boston dataset, target is typically 'medv' or 'MEDV'
              target_col = 'medv' if 'medv' in df.columns else 'MEDV'
              X = df.drop(columns=[target_col])
              y = df[target_col]

              # Standardize features using StandardScaler
              scaler = StandardScaler()
              X_scaled = scaler.fit_transform(X)
              X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

              # Split into train/test
              X_train, X_test, y_train, y_test = train_test_split(
                  X_scaled, y, test_size=0.2, random_state=42
              )

              # Prepare data dictionary to save
              processed_data = {
                  'X_train': X_train,
                  'X_test': X_test,
                  'y_train': y_train,
                  'y_test': y_test,
                  'scaler': scaler,
                  'feature_names': X.columns.tolist()
              }

              # Save processed.pkl using joblib
              joblib.dump(processed_data, processed_path)

              # Log parameters
              mlflow.log_param("source_file", str(raw_path))
              mlflow.log_param("test_size", 0.2)
              mlflow.log_param("random_state", 42)
              mlflow.log_param("n_features", len(X.columns))
              mlflow.log_param("n_samples", len(df))
              mlflow.log_param("n_train", len(X_train))
              mlflow.log_param("n_test", len(X_test))

              # Log artifacts
              mlflow.log_artifact(str(processed_path))

              # Log metrics
              mlflow.log_metric("train_samples", len(X_train))
              mlflow.log_metric("test_samples", len(X_test))

          return str(processed_path)

      def _serialize_str(str_value: str) -> str:
          if not isinstance(str_value, str):
              raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                  str(str_value), str(type(str_value))))
          return str_value

      import argparse
      _parser = argparse.ArgumentParser(prog='Preprocess data', description='Preprocess data: load CSV, standardize features, split train/test, and save.')
      _parser.add_argument("--raw-path", dest="raw_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--processed-path", dest="processed_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
      _parsed_args = vars(_parser.parse_args())
      _output_files = _parsed_args.pop("_output_paths", [])

      _outputs = preprocess_data(**_parsed_args)

      _outputs = [_outputs]

      _output_serializers = [
          _serialize_str,

      ]

      import os
      for idx, output_file in enumerate(_output_files):
          try:
              os.makedirs(os.path.dirname(output_file))
          except OSError:
              pass
          with open(output_file, 'w') as f:
              f.write(_output_serializers[idx](_outputs[idx]))
    args:
    - --raw-path
    - {inputValue: raw_path}
    - --processed-path
    - {inputValue: processed_path}
    - '----output-paths'
    - {outputPath: Output}
